
# While attempting the test, DO NOT EDIT this file. It may result in 0 score.

import subprocess
import yaml
from pathlib import Path
filepath = Path(__file__)
root = filepath.parent
#root = filepath.parents[1]

# Initial score
score = 0

print("--------- Preprocess the Data (`data_preprocessing.py`) [2 Marks] ---------")

# Data preprocessing points
try:
    from data_preprocessing import X_train, X_test, y_train, y_test
    score += 1
    print("Points given for train-test-split step: 1")
except:
    print("[ERROR]: `from data_preprocessing import X_train, X_test, y_train, y_test` <-- NOT WORKING")
    pass

# Check for saved `label_encoders.pkl` & `rf_model_term_deposit.pkl`
encoder_file = Path(str(root) + "/" + "trained_model/label_encoders.pkl")

if encoder_file.is_file():
    score += 1
    print("Points given for saving label_encoders.pkl: 1")
else:
    print("[ERROR]: `trained_model/label_encoders.pkl` <-- FILE DOES NOT EXIST")

print("--------- Train a Machine Learning Model (`train_model.py`) [4 Marks] ---------")

model_file = Path(str(root) + "/" + "trained_model/rf_model_term_deposit.pkl")

if model_file.is_file():
    score += 1
    print("Points given for saving rf_model_term_deposit.pkl: 1")
else:
    print("[ERROR]: `trained_model/rf_model_term_deposit.pkl` <-- FILE DOES NOT EXIST")


# Check for logged parameter in mlflow
exp_path = Path(str(root) + "/" + "mlruns/0")

if exp_path.exists():      # if mlruns/0 exists

    exp_dict = {}

    for subpath in exp_path.iterdir():         # 0/*
        if subpath.is_dir():
            for subdir in subpath.iterdir():
                if subdir.is_file():              # 0/*/meta.yaml
                    # Load the YAML file
                    with open(subdir, 'r') as file:
                        data = yaml.safe_load(file)
                        start_time = data['start_time']
                        exp_dict[start_time] = subpath
                        # print(subpath, start_time)

    run_path = exp_dict[max(exp_dict.keys())]      # latest run
    #print("run path:", run_path)
    
    # Check the run folder for logged params, metrics, artifacts
    for subdir in run_path.iterdir():
        if subdir.is_dir():
            if subdir.name == 'params':
                if any(subdir.iterdir()):     # if directory is not empty
                    score += 1
                    print("Points given for logging parameters with mlflow: 1")
                else:
                    print("[ERROR]: Parameters NOT LOGGED with MLflow")
            if subdir.name == 'metrics': 
                if any(subdir.iterdir()):     # if directory is not empty
                    score += 1
                    print("Points given for logging metrics with mlflow: 1")
                else:
                    print("[ERROR]: Metrics NOT LOGGED with MLflow")
            if subdir.name == 'artifacts':    # if directory is not empty
                if any(subdir.iterdir()):
                    for artifact_file in subdir.iterdir():
                        if artifact_file.is_dir():
                            for sub_art_file in artifact_file.iterdir():
                                if sub_art_file.is_file() and sub_art_file.suffix == '.pkl':  # check for logged model
                                    score += 1
                                    print("Points given for logging model pkl with mlflow: 1")
                else:
                    print("[ERROR]: Model NOT LOGGED with MLflow")

else:
    print("[ERROR]: LOGGING with MLflow NOT IMPLEMENTED")


# Evaluate Inference - predict.py
print("--------- Make Predictions - Inference (`predict.py`) [2 Marks] ---------")

# Check for `predict.py`
try:
    from predict import rf_model
    score += 1
    print("Points given for loading the model pkl: 1")
except:
    pass

try:
    from predict import make_prediction, sample_input_df
    sample_pred = make_prediction(sample_input_df)
    if sample_pred in ["Subscribed (y=1)", "Not Subscribed (y=0)"]:
        score += 1
        print("Points given for make_prediction() func: 1")
except:
    pass


# Evaluate pytest tests
print("--------- Build Test Cases (`test/test_prediction.py`) [4 Marks] ---------")

# Check for test_model_accuracy() test case:
try:
    result = subprocess.run(['pytest', 'tests/test_prediction.py::test_model_accuracy'], capture_output=True, text=True)
    #print(result.stdout)
    #print(result.stderr)
    if result.returncode == 0:
        score += 2
        print("Points given for test_model_accuracy() func: 2")
    else:
        print("[ERROR]: `test_model_accuracy()` <-- TEST FAILED or NOT DEFINED YET")
except Exception as e:
    print(f"[ERROR]: Error running pytest: {e}")


# Check for test_make_prediction() test case:
try:
    result = subprocess.run(['pytest', 'tests/test_prediction.py::test_make_prediction_function'], capture_output=True, text=True)
    #print(result.stdout)
    #print(result.stderr)
    if result.returncode == 0:
        score += 2
        print("Points given for test_make_prediction_function() func: 2")
    else:
        print("[ERROR]: `test_make_prediction_function()` <-- TEST FAILED or NOT DEFINED YET")
except Exception as e:
    print(f"[ERROR]: Error running pytest: {e}")


# Serve the Model via REST API using FastAPI (`app.py`) [4 Marks]
print("--------- Serve the Model via REST API using FastAPI (`app.py`) [4 Marks] ---------")

try:
    install_httpx = subprocess.run(['pip', 'install', 'httpx'], capture_output=True, text=True)
    # test_app.py  `pip install httpx`
    from fastapi.testclient import TestClient
    from app import app

    client = TestClient(app)

    payload = {
        'age': 33,
        'job': 'technician',
        'marital': 'single',
        'education': 'secondary',
        'default': 0,
        'balance': 1500,
        'housing': 1,
        'loan': 0,
        'day_of_week': 'mon',
        'month': 'may',
        'duration': 120,
        'campaign': 2,
        'pdays': -1,
        'previous': 0
    }

    response = client.post("/predict", json=payload)

    if response.json()["prediction"] in ["Subscribed (y=1)", "Not Subscribed (y=0)"]:
        score += 4
        print("Points given for FastAPI implementation: 4")
    elif response.json()["prediction"] == "Update this variable":
        print("[ERROR]: `app.py` <-- FastAPI part NOT IMPLEMENTED YET")
    else:
        print("[ERROR]: Error in FastAPI implementation `app.py`")

except:
    print("[ERROR]: Error in FastAPI implementation `app.py`. May be `/predict` endpoint NOT DEFINED")
    pass


# Dockerize the FastAPI Application [4 Marks]
print("--------- Dockerize the FastAPI Application [4 Marks] ---------")



if score < 0:
    score_pct = 0
elif score > 20:
    score_pct = 100
else:
    score_pct = (score / 20) * 100

print("------------------------")
print(f"Score given for challenge: {score}/20")
print(f"Partial Credit: {score_pct}%")
